{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7GqFjrGmLgT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87207ce0-236f-4304-db8e-e566edc8c0c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Installing torchprofile...')\n",
        "!pip install torchprofile 1>/dev/null\n",
        "print('All required packages have been successfully installed!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiT3yl_1rXt6",
        "outputId": "aa118415-03db-469e-e4f9-58c3be7a8003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing torchprofile...\n",
            "All required packages have been successfully installed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "from collections import OrderedDict, defaultdict\n",
        "from typing import Union, List\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "from torch import nn\n",
        "from torch.optim import *\n",
        "from torch.optim.lr_scheduler import *\n",
        "from torch.utils.data import DataLoader\n",
        "from torchprofile import profile_macs\n",
        "from torchvision.datasets import *\n",
        "from torchvision.transforms import *\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from torchprofile import profile_macs"
      ],
      "metadata": {
        "id": "3Ry22siprR0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YG0ZE948rSDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class ModelPruner:\n",
        "    def __init__(self, model: nn.Module, dataloader: dict, device: torch.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
        "        \"\"\"\n",
        "        Initialize the ModelPruner class.\n",
        "\n",
        "        Parameters:\n",
        "        - model: The neural network model to prune.\n",
        "        - dataloader: A dictionary containing 'train' and 'test' DataLoader objects.\n",
        "        - device: The device to run the model on ('cuda' or 'cpu').\n",
        "        \"\"\"\n",
        "        self.model = model.to(device)\n",
        "        self.dataloader = dataloader\n",
        "        self.device = device\n",
        "\n",
        "    def count_parameters(self, model: nn.Module, count_nonzero_only=False) -> int:\n",
        "        \"\"\"Count the parameters in the model.\"\"\"\n",
        "        return sum(p.numel() for p in model.parameters() if p.requires_grad and (p.count_nonzero() if count_nonzero_only else True))\n",
        "\n",
        "    def get_model_size(self, model: nn.Module, count_nonzero_only=False) -> int:\n",
        "        \"\"\"Calculate the model size.\"\"\"\n",
        "        # Assuming 32 bits (4 bytes) per parameter\n",
        "        return self.count_parameters(model, count_nonzero_only) * 4\n",
        "\n",
        "    def evaluate(self, model: nn.Module) -> float:\n",
        "        \"\"\"Evaluate the model accuracy on the test dataset.\"\"\"\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data in self.dataloader['test']:\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "        return 100 * correct / total\n",
        "\n",
        "    @staticmethod\n",
        "    def create_pruning_mask(tensor: torch.Tensor, target_sparsity: float) -> torch.Tensor:\n",
        "        \"\"\"Create a binary mask for pruning.\"\"\"\n",
        "        tensor_flat = tensor.view(-1)\n",
        "        threshold = torch.quantile(torch.abs(tensor_flat), target_sparsity)\n",
        "        mask = torch.abs(tensor) > threshold\n",
        "        return mask.float()\n",
        "\n",
        "    def fine_grained_prune(self, target_sparsity: float):\n",
        "        \"\"\"Apply fine-grained pruning to the model.\"\"\"\n",
        "        for param in self.model.parameters():\n",
        "            if param.requires_grad and param.dim() > 1:  # Typically for weights of conv and linear layers\n",
        "                mask = self.create_pruning_mask(param.data, target_sparsity)\n",
        "                param.data.mul_(mask)\n",
        "\n",
        "    def fine_tune(self, epochs: int, lr: float = 0.01):\n",
        "        \"\"\"Fine-tune the pruned model.\"\"\"\n",
        "        optimizer = torch.optim.SGD(self.model.parameters(), lr=lr, momentum=0.9)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        self.model.train()\n",
        "        for epoch in range(epochs):\n",
        "            for data in self.dataloader['train']:\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "    def prune_and_fine_tune(self, target_sparsity: float, fine_tune_epochs: int):\n",
        "        \"\"\"Prune the model and then fine-tune it.\"\"\"\n",
        "        print(\"Initial Model Evaluation:\")\n",
        "        initial_accuracy = self.evaluate(self.model)\n",
        "        initial_size = self.get_model_size(self.model)\n",
        "        initial_params = self.count_parameters(self.model)\n",
        "        print(f\"Accuracy: {initial_accuracy:.2f}%, Size: {initial_size} bytes, Parameters: {initial_params}\")\n",
        "\n",
        "        print(\"\\nPruning Model...\")\n",
        "        self.fine_grained_prune(target_sparsity)\n",
        "\n",
        "        print(\"Model Evaluation After Pruning:\")\n",
        "        pruned_accuracy = self.evaluate(self.model)\n",
        "        pruned_size = self.get_model_size(self.model, count_nonzero_only=True)\n",
        "        pruned_params = self.count_parameters(self.model, count_nonzero_only=True)\n",
        "        print(f\"Accuracy: {pruned_accuracy:.2f}%, Size: {pruned_size} bytes, Parameters: {pruned_params}\")\n",
        "\n",
        "        print(\"\\nFine-tuning Model...\")\n",
        "        self.fine_tune(fine_tune_epochs)\n",
        "\n",
        "        print(\"Final Model Evaluation:\")\n",
        "        final_accuracy = self.evaluate(self.model)\n",
        "        final_size = self.get_model_size(self.model, count_nonzero_only=True)\n",
        "        final_params = self.count_parameters(self.model, count_nonzero_only=True)\n",
        "        print(f\"Accuracy: {final_accuracy:.2f}%, Size: {final_size} bytes, Parameters: {final_params}\")\n"
      ],
      "metadata": {
        "id": "qbWw4T8kmW73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_url(url, model_dir='.', overwrite=False):\n",
        "    import os, sys, ssl\n",
        "    from urllib.request import urlretrieve\n",
        "    ssl._create_default_https_context = ssl._create_unverified_context\n",
        "    target_dir = url.split('/')[-1]\n",
        "    model_dir = os.path.expanduser(model_dir)\n",
        "    try:\n",
        "        if not os.path.exists(model_dir):\n",
        "            os.makedirs(model_dir)\n",
        "        model_dir = os.path.join(model_dir, target_dir)\n",
        "        cached_file = model_dir\n",
        "        if not os.path.exists(cached_file) or overwrite:\n",
        "            sys.stderr.write('Downloading: \"{}\" to {}\\n'.format(url, cached_file))\n",
        "            urlretrieve(url, cached_file)\n",
        "        return cached_file\n",
        "    except Exception as e:\n",
        "        # remove lock file so download can be executed next time.\n",
        "        os.remove(os.path.join(model_dir, 'download.lock'))\n",
        "        sys.stderr.write('Failed to download from url %s' % url + '\\n' + str(e) + '\\n')\n",
        "        return None"
      ],
      "metadata": {
        "id": "6ZGzFaWXp1hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG(nn.Module):\n",
        "  ARCH = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
        "\n",
        "  def __init__(self) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    layers = []\n",
        "    counts = defaultdict(int)\n",
        "\n",
        "    def add(name: str, layer: nn.Module) -> None:\n",
        "      layers.append((f\"{name}{counts[name]}\", layer))\n",
        "      counts[name] += 1\n",
        "\n",
        "    in_channels = 3\n",
        "    for x in self.ARCH:\n",
        "      if x != 'M':\n",
        "        # conv-bn-relu\n",
        "        add(\"conv\", nn.Conv2d(in_channels, x, 3, padding=1, bias=False))\n",
        "        add(\"bn\", nn.BatchNorm2d(x))\n",
        "        add(\"relu\", nn.ReLU(True))\n",
        "        in_channels = x\n",
        "      else:\n",
        "        # maxpool\n",
        "        add(\"pool\", nn.MaxPool2d(2))\n",
        "\n",
        "    self.backbone = nn.Sequential(OrderedDict(layers))\n",
        "    self.classifier = nn.Linear(512, 10)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    # backbone: [N, 3, 32, 32] => [N, 512, 2, 2]\n",
        "    x = self.backbone(x)\n",
        "\n",
        "    # avgpool: [N, 512, 2, 2] => [N, 512]\n",
        "    x = x.mean([2, 3])\n",
        "\n",
        "    # classifier: [N, 512] => [N, 10]\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Kh5LHP6MqRV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(\n",
        "  model: nn.Module,\n",
        "  dataloader: DataLoader,\n",
        "  criterion: nn.Module,\n",
        "  optimizer: Optimizer,\n",
        "  scheduler: LambdaLR,\n",
        "  callbacks = None\n",
        ") -> None:\n",
        "  model.train()\n",
        "\n",
        "  for inputs, targets in tqdm(dataloader, desc='train', leave=False):\n",
        "    # Move the data from CPU to GPU\n",
        "    inputs = inputs.cuda()\n",
        "    targets = targets.cuda()\n",
        "\n",
        "    # Reset the gradients (from the last iteration)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward inference\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "\n",
        "    # Backward propagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Update optimizer and LR scheduler\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    if callbacks is not None:\n",
        "        for callback in callbacks:\n",
        "            callback()"
      ],
      "metadata": {
        "id": "l8o_bNQHqhvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_url = \"https://hanlab18.mit.edu/files/course/labs/vgg.cifar.pretrained.pth\"\n",
        "checkpoint = torch.load(download_url(checkpoint_url), map_location=\"cpu\")\n",
        "model = VGG()\n",
        "print(f\"=> loading checkpoint '{checkpoint_url}'\")\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "recover_model = lambda: model.load_state_dict(checkpoint['state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hatpZzTEniMI",
        "outputId": "2d98599c-f56a-4551-a736-b2bc2f187c4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://hanlab18.mit.edu/files/course/labs/vgg.cifar.pretrained.pth\" to ./vgg.cifar.pretrained.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> loading checkpoint 'https://hanlab18.mit.edu/files/course/labs/vgg.cifar.pretrained.pth'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define transformations\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # Normalizing the images to [-1, 1]\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # Normalizing the images to [-1, 1]\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "# Define DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# Your existing model initialization and training code follows here\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GPqLlI3-aB5",
        "outputId": "4ca4ead1-2b17-401b-cde4-20ab0b5b5ab2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:11<00:00, 15337622.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume model, train_loader, and test_loader are predefined\n",
        "dataloader = {'train': train_loader, 'test': test_loader}\n",
        "pruner = ModelPruner(model, dataloader)\n",
        "pruner.prune_and_fine_tune(target_sparsity=0.95, fine_tune_epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XGvlAVqmfke",
        "outputId": "7cfc9134-8d51-4e5d-b09c-15221b017233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Model Evaluation:\n",
            "Accuracy: 89.48%, Size: 36913448 bytes, Parameters: 9228362\n",
            "\n",
            "Pruning Model...\n",
            "Model Evaluation After Pruning:\n",
            "Accuracy: 10.04%, Size: 36913448 bytes, Parameters: 9228362\n",
            "\n",
            "Fine-tuning Model...\n",
            "Final Model Evaluation:\n",
            "Accuracy: 89.39%, Size: 36913448 bytes, Parameters: 9228362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kWFRMj8V5Ofo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}