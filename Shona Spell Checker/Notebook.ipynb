{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for loading the files\n",
    "def load_files(folder_path):\n",
    "    \n",
    "    text_data = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".sgm\") or filename.endswith(\".xml\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                soup = BeautifulSoup(file, 'html.parser')\n",
    "                text = soup.get_text()\n",
    "                words = word_tokenize(text)\n",
    "                text_data.extend(words)\n",
    "\n",
    "    return list(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['akati', ',', '``', 'Mvura', 'iri', 'pasi', 'pedenga', 'ngaiungane', 'pamwe', 'chete', 'kuti', 'pasi', 'pakaoma', 'pagoonekwa', '.', \"''\", 'Zvikaita', 'saizvozvo', '.', '10', 'Mwari', 'akatumidza', 'pasi', 'pakaoma', 'kuti', '``', 'Nyika', \"''\", 'uye', 'mvura', 'yakanga', 'yakaungana', ',', 'akaitumidza', 'kuti', '``', 'Makungwa', \"''\", '.', 'Mwari', 'akaona', 'kuti', 'zvakanaka', '.', '11', 'Mwari', 'akati', ',', '``', 'Nyika']\n"
     ]
    }
   ],
   "source": [
    "folder_path = r\"C:\\Users\\LENOVO\\Documents\\Capstone Project\\Datasets\"\n",
    "text_data = load_files(folder_path)\n",
    "\n",
    "# Display a few words from the dataset\n",
    "print(text_data[200:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('words')\n",
    "english_words = set(words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(words):\n",
    "\n",
    "    #removing utf-8 encoding characters and other unnecessary chars\n",
    "    words = [re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\xff\\xad\\x0c6§\\[\\]\\\\\\£\\Â\\n\\r]', ' ', word) for word in words]\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "\n",
    "    #removing all numerical values(page numbers, verses, chapters etc)\n",
    "    words = [re.sub(r'[0123456789]', ' ', word) for word in words]\n",
    "\n",
    "    #removing all punctuation\n",
    "    re_punc = re.compile( ' [%s] ' % re.escape(string.punctuation))\n",
    "    words = [re_punc.sub( '' , word) for word in words]\n",
    "\n",
    "    #removing Roman numerals\n",
    "    # first capitalized ones\n",
    "    words = [re.sub(r'\\s((I{2,}V*X*\\.*)|(IV\\.*)|(IX\\.*)|(V\\.*)|(V+I*\\.*)|(X+L*V*I*]\\.*))\\s', ' ', word) for word in words]\n",
    "    # then lowercase\n",
    "    words = [re.sub(r'\\s((i{2,}v*x*\\.*)|(iv\\.*)|(ix\\.*)|(v\\.*)|(v+i*\\.*)|(x+l*v*i*\\.*))\\s', ' ', word) for word in words]\n",
    "\n",
    "    #removing all strings of capital letters that are more than 2 characters long(Headings)\n",
    "    words = [re.sub(r'[A-Z]{2,}', ' ', word) for word in words]\n",
    "\n",
    "    #removing extra white spaces\n",
    "    words = [re.sub(r'\\s+', ' ', word) for word in words]\n",
    "\n",
    "    #removing English words  from the corpus\n",
    "    words = [word for word in words if word not in english_words]\n",
    "\n",
    "    #converting all words to lower case\n",
    "    clean_words = [word.lower() for word in words]\n",
    "\n",
    "    #removing empty strings\n",
    "    for word in clean_words:\n",
    "        if((len(word)) == 1):\n",
    "            clean_words.remove(word)\n",
    "\n",
    "    return clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nezviedza',\n",
       " 'kuti',\n",
       " 'zvisiyanise',\n",
       " 'masikati',\n",
       " 'nousiku',\n",
       " 'zvigova',\n",
       " 'zviratidzo',\n",
       " 'zvenguva',\n",
       " 'nezvamazuva',\n",
       " 'ngazvive']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text = preprocess(text_data)\n",
    "clean_text[200:210]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing English Words from the corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_dict_path = r\"C:\\Users\\LENOVO\\Downloads\\Oxford English Dictionary.txt\"\n",
    "\n",
    "with open(eng_dict_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "    eng_dict = [word.strip() for word in file.readlines()]\n",
    "eng_dict = word_tokenize(str(eng_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m clean_shona_text \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m clean_text \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m eng_dict]\n\u001b[0;32m      2\u001b[0m clean_shona_text[:\u001b[38;5;241m10\u001b[39m]\n",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m clean_shona_text \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m clean_text \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m eng_dict]\n\u001b[0;32m      2\u001b[0m clean_shona_text[:\u001b[38;5;241m10\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clean_shona_text = [word for word in clean_text if word not in eng_dict]\n",
    "clean_shona_text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the clean text to a file, for future reference\n",
    "def save_to_file(clean_words, output_file_path):\n",
    "    \n",
    "    with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write('\\n'.join(clean_words))\n",
    "\n",
    "\n",
    "output_file_path = r\"C:\\Users\\LENOVO\\Documents\\Capstone Project\\clean_text.txt\"\n",
    "save_to_file(clean_shona_text, output_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation: Noisemaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def introduce_keyboard_errors(word, max_modifications=2):\n",
    "    keyboard_layout = {\n",
    "        'a': 'qwsz',\n",
    "        'b': 'vghn',\n",
    "        'c': 'xdfv',\n",
    "        'd': 'erfcxs',\n",
    "        'e': 'rdsw',\n",
    "        'f': 'rtgvcxd',\n",
    "        'g': 'tyhbvf',\n",
    "        'h': 'yujnbg',\n",
    "        'i': 'uokj',\n",
    "        'j': 'uikmnh',\n",
    "        'k': 'iojlm',\n",
    "        'l': 'opk',\n",
    "        'm': 'njk',\n",
    "        'n': 'bhjm',\n",
    "        'o': 'iplk',\n",
    "        'p': 'olo',\n",
    "        'q': 'wa',\n",
    "        'r': 'tfe',\n",
    "        's': 'awedxz',\n",
    "        't': 'ygr',\n",
    "        'u': 'ijyh',\n",
    "        'v': 'cfgb',\n",
    "        'w': 'qasde',\n",
    "        'x': 'zsdc',\n",
    "        'y': 'uhtg',\n",
    "        'z': 'asx'\n",
    "    }\n",
    "\n",
    "    noisy_word = word\n",
    "    for _ in range(random.randint(1, max_modifications)):\n",
    "        # Select a random position in the word\n",
    "        position = random.randint(0, len(word) - 1)\n",
    "        # Simulate keyboard layout error by randomly selecting a nearby key\n",
    "        noisy_char = random.choice(keyboard_layout.get(word[position], word[position]))\n",
    "        # Modify the word at the selected position\n",
    "        noisy_word = noisy_word[:position] + noisy_char + noisy_word[position + 1:]\n",
    "\n",
    "    return noisy_word\n",
    "\n",
    "def create_dataset(correct_words, num_mispellings=3, max_modifications=2):\n",
    "    dataset = {\"Incorrect Spelling\": [], \"Correct Spelling\": []}\n",
    "\n",
    "    for word in correct_words:\n",
    "        # Generate multiple misspellings for each word\n",
    "        for _ in range(num_mispellings):\n",
    "            mispelled_word = introduce_keyboard_errors(word, max_modifications)\n",
    "            dataset[\"Incorrect Spelling\"].append(mispelled_word)\n",
    "            dataset[\"Correct Spelling\"].append(word)\n",
    "\n",
    "    #create a dataframe\n",
    "    return pd.DataFrame(dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Incorrect Spelling  Correct Spelling\n",
      "0             muzadae           muzadze\n",
      "1             muzadzr           muzadze\n",
      "2             muzadzr           muzadze\n",
      "3             muaadzs           muzadze\n",
      "4             muaadze           muzadze\n",
      "5               mvira             mvura\n",
      "6               mfura             mvura\n",
      "7               nvura             mvura\n",
      "8               mcurz             mvura\n",
      "9               mvhra             mvura\n",
      "10         yomugunvwa        yomugungwa\n",
      "11         ykmugungwa        yomugungwa\n",
      "12         yomugubgwa        yomugungwa\n",
      "13         yonugunywa        yomugungwa\n",
      "14         uomugungwa        yomugungwa\n",
      "15                yys               uye\n",
      "16                hyd               uye\n",
      "17                iyw               uye\n",
      "18                ute               uye\n",
      "19                jye               uye\n",
      "20              shkro             shiri\n",
      "21              shifj             shiri\n",
      "22              zhiri             shiri\n",
      "23              shjri             shiri\n",
      "24              ehuri             shiri\n",
      "25   ngadziberekanewp  ngadziberekanewo\n",
      "26   ngadziberekanswo  ngadziberekanewo\n",
      "27   bgadziberekanewo  ngadziberekanewo\n",
      "28   ngadzigerekamewo  ngadziberekanewo\n",
      "29   ngqdzibeeekanewo  ngadziberekanewo\n"
     ]
    }
   ],
   "source": [
    "# Testing the function on a small sample \n",
    "correct_words = clean_text[300:306]\n",
    "spell_checker_dataset = create_dataset(correct_words, num_mispellings=5, max_modifications=2)\n",
    "\n",
    "# Display the dataset\n",
    "print(spell_checker_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
