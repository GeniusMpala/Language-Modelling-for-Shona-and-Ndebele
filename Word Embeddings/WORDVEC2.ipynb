{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import regex as re\n",
    "import multiprocessing\n",
    "from gensim.models.callbacks import CallbackAny2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Shona text data from a file\n",
    "with open('shona_corpus.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GPT-4 tokenization pattern\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "def shona_tokenize(text):\n",
    "    # Use regex to tokenize Shona text\n",
    "    tokens = re.findall(GPT4_SPLIT_PATTERN, text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Shona text\n",
    "word_tokens = [shona_tokenize(line) for line in lines]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1232529"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Word2Vec parameters\n",
    "vector_size = 100  # Dimensionality of word vectors\n",
    "window = 5  # Maximum distance between the current and predicted word within a sentence\n",
    "min_count = 1  # Ignores all words with a total frequency lower than this\n",
    "epochs = 200 # Number of training epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model for multiple epochs\n",
    "model = Word2Vec(vector_size=vector_size,sentences=word_tokens,min_count=min_count, epochs=epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model.save(\"shona_word2vec_100v.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.4800901  -2.4462605  -0.5620928  -4.0146427  -5.931522   -2.5026622\n",
      " -6.7239256   0.84800726  3.5956128   1.3697836   2.5355656   3.0576656\n",
      "  1.1785955  -1.2922791  -1.1786398   4.184401    0.08780147 -0.3715545\n",
      " -0.8753085   1.2553934  -1.8640904   5.280042    1.2764175  -1.1019186\n",
      "  0.27422938  0.58728844  0.16113763  0.15737386  0.3983518   8.344677\n",
      "  1.0794947  -0.32144943 -6.830682    2.3552854   2.6356559   1.1357268\n",
      " -1.6503016  -2.6065474  -2.4918022  -0.49499315  0.14990868  0.24240625\n",
      " -4.1462307  -0.5355225   2.9920757   5.9263325   4.932553   -2.9086018\n",
      " -7.3028603  -3.0883338  -0.95288455  0.2485519   4.6712794   4.390667\n",
      "  0.49413794  2.315377   -3.0482666   1.5172665  -7.3865848  -1.4324028\n",
      "  2.341667   -4.2083187   1.7560204   4.463658    1.0819036   2.28142\n",
      " -0.8031522   2.2467227  -5.75244    -2.209407    3.570298   -0.9275468\n",
      " -5.4589114  -0.4458149   2.2662156   2.7048895   0.3646489  -3.4331353\n",
      " -1.3079004   1.6441617  -0.9167239   2.2608063  -2.0626817  -0.39179233\n",
      " -2.9221618  -1.3886894   0.7705892   0.79328567  1.7506505  -0.7152359\n",
      " -0.6504664  -1.7902781  -4.573021    1.2206866   2.8336172   2.3547554\n",
      "  0.1785916  -2.079609   -1.2573545   2.3932955 ]\n"
     ]
    }
   ],
   "source": [
    "# Example usage of word vectors\n",
    "print(model.wv['mwari'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=57115, vector_size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = Word2Vec.load('shona_word2vec_300v.bin')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('munonyatsa', 0.38684016466140747), ('kwechinguva', 0.386617511510849), ('ndaizodzoka', 0.3853031396865845)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "# Example of similarity operation\n",
    "result = model.wv.most_similar(positive=['musikana', 'baba'], negative=['mukomana'], topn=3)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word analogy: 'mambo' is to 'murume' as 'mukadzi' is to: [('charakupa', 0.39902248978614807), ('dzinomisawo', 0.3958755135536194), ('pamusuo', 0.3640866279602051)]\n"
     ]
    }
   ],
   "source": [
    "# Test solving word analogies\n",
    "word1 = 'mambo'\n",
    "word2 = 'murume'\n",
    "word3 = 'mukadzi'\n",
    "analogy_result = model.wv.most_similar(positive=[word1, word3], negative=[word2], topn=3)\n",
    "print(f\"Word analogy: '{word1}' is to '{word2}' as '{word3}' is to: {analogy_result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv['mambo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to 'amai': [('kunotinzi', 0.43546196818351746), ('murimuka', 0.36470407247543335), ('achazarura', 0.3626631200313568), ('zvinonzizve', 0.36215704679489136), ('kungonokanda', 0.3575122654438019), ('musazoita', 0.3566194176673889), ('chigidi', 0.3547627031803131), ('angazofara', 0.3530261218547821), ('tinetanga', 0.35171061754226685), ('ngatirovei', 0.3508082628250122)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "# Test finding most similar words\n",
    "word = 'amai'\n",
    "most_similar_words = model.wv.most_similar(word)\n",
    "print(f\"Most similar words to '{word}': {most_similar_words}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for the next word in 'mavambo kusikwa kwezvinhu zvose pakutanga mwari ': [('mwari', 1.0), ('wamwewo', 0.3974516689777374), ('ndakazoombekwa', 0.3922135829925537), ('kutoseka', 0.3852808177471161), ('ndoopakaperera', 0.36315077543258667)]\n"
     ]
    }
   ],
   "source": [
    "def predict_next_word(sentence, model, topn=5):\n",
    "    # Tokenize the input sentence\n",
    "    tokens = sentence.split()\n",
    "    \n",
    "    # Get the vector representation of the last word in the sentence\n",
    "    if tokens[-1] in model.wv:\n",
    "        last_word_vector = model.wv[tokens[-1]]\n",
    "    else:\n",
    "        print(f\"Word '{tokens[-1]}' not found in the vocabulary.\")\n",
    "        return None\n",
    "    \n",
    "    # Find the most similar words to the vector of the last word\n",
    "    most_similar_words = model.wv.similar_by_vector(last_word_vector, topn=topn)\n",
    "    \n",
    "    return most_similar_words\n",
    "\n",
    "# Example usage\n",
    "sentence = \"mavambo kusikwa kwezvinhu zvose pakutanga mwari \"\n",
    "next_word_predictions = predict_next_word(sentence, new_model)\n",
    "print(f\"Predictions for the next word in '{sentence}': {next_word_predictions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
